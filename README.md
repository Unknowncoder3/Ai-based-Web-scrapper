ğŸ¤– AI-Powered Web Scraper with FAISS & Local LLM (RAG)

An AI-powered web scraping and question-answering system built with Streamlit, FAISS, and local LLMs via Ollama.
The application scrapes website content, converts it into vector embeddings, stores it in a FAISS vector database, and allows users to ask natural-language questions using a Retrieval-Augmented Generation (RAG) approach.

ğŸš€ Features

ğŸŒ Scrape textual content from any public website

âœ‚ï¸ Automatic text chunking for efficient processing

ğŸ”¢ Semantic embeddings using Hugging Face Sentence Transformers

âš¡ Fast similarity search with FAISS

ğŸ¤– Local LLM inference using Ollama (Mistral, LLaMA, etc.)

ğŸ§  Context-aware question answering (RAG)

ğŸ–¥ï¸ Interactive Streamlit web interface

ğŸ”’ Fully local & offline (after model download)

ğŸ§  Architecture Overview
User â†’ Streamlit UI
        â†“
   Website URL
        â†“
  Requests + BeautifulSoup
        â†“
   Clean Text Extraction
        â†“
 Character Text Splitter
        â†“
 HuggingFace Embeddings
        â†“
     FAISS Vector Store
        â†“
 Similarity Search (Query)
        â†“
 Retrieved Context
        â†“
     Ollama LLM
        â†“
   Final AI Answer

ğŸ› ï¸ Tech Stack

Python

Streamlit â€“ Web UI

Requests â€“ HTTP requests

BeautifulSoup4 â€“ HTML parsing

FAISS â€“ Vector similarity search

NumPy â€“ Numerical operations

Hugging Face Sentence Transformers â€“ Text embeddings

LangChain (Modular packages) â€“ LLM & embeddings interface

Ollama â€“ Local LLM execution (Mistral / LLaMA)
